<!DOCTYPE html>
<html lang="en">
<head>
    <title>Image Rendering with WebGL Shader</title>
</head>
<body>
<canvas id="canvas" width="640" height="360"></canvas>
<video autoplay id="selfVideo"></video>
<!-- debug -->
<canvas id="c" width="640" height="360"></canvas>
<script type="module">
    const vs = `
            attribute vec2 a_position;
            varying vec2 v_texcoord;
            void main() {
                gl_Position = vec4(a_position, 0.0, 1.0);
                v_texcoord = (a_position + 1.0) / 2.0 * vec2(1, -1);
            }
        `;

    const fs = `
            precision mediump float;
            varying vec2 v_texcoord;
            uniform sampler2D u_texture;
            uniform sampler2D uv_texture;

            vec4 getRgb(vec2 coord) {
                float y = texture2D(u_texture, coord).x;
                float u = texture2D(uv_texture, coord).x - 0.5;
                float v = texture2D(uv_texture, coord).a - 0.5;

                // Convert YUV to RGB
                return vec4(
                    y + 1.13983 * v,              // R
                    y - 0.39465 * u - 0.5806 * v, // G
                    y + 2.03211 * u,              // B
                    1.0
                );
            }

            void main() {
                gl_FragColor = getRgb(v_texcoord);
            }
        `;

    const canvas = document.getElementById('canvas');
    const gl = canvas.getContext('webgl2', { alpha: false, antialias: false, colorSpace: "srgb" });
    // debug
    const ctx = window.c.getContext('2d');
    window.ctx = ctx;

    if (!gl) throw new Error('WebGL2 is not supported');

    const vertexShader = gl.createShader(gl.VERTEX_SHADER);
    gl.shaderSource(vertexShader, vs);
    gl.compileShader(vertexShader);

    const fragmentShader = gl.createShader(gl.FRAGMENT_SHADER);
    gl.shaderSource(fragmentShader, fs);
    gl.compileShader(fragmentShader);

    const shaderProgram = gl.createProgram();
    gl.attachShader(shaderProgram, vertexShader);
    gl.attachShader(shaderProgram, fragmentShader);
    gl.linkProgram(shaderProgram);
    gl.useProgram(shaderProgram);

    const vertices = new Float32Array([-1, -1, 1, -1, -1, 1, 1, 1]);
    const vertexBuffer = gl.createBuffer();
    gl.bindBuffer(gl.ARRAY_BUFFER, vertexBuffer);
    gl.bufferData(gl.ARRAY_BUFFER, vertices, gl.STATIC_DRAW);

    const positionLocation = gl.getAttribLocation(shaderProgram, 'a_position');
    gl.enableVertexAttribArray(positionLocation);
    gl.vertexAttribPointer(positionLocation, 2, gl.FLOAT, false, 0, 0);


    gl.activeTexture(gl.TEXTURE0);
    const texture = gl.createTexture();
    gl.bindTexture(gl.TEXTURE_2D, texture);
    // Set the texture uniform in the shader
    const textureLocation = gl.getUniformLocation(shaderProgram, 'u_texture');
    gl.uniform1i(textureLocation, 0);


    gl.activeTexture(gl.TEXTURE1);
    const texture_uv = gl.createTexture();
    gl.bindTexture(gl.TEXTURE_2D, texture_uv);
    const textureLocation2 = gl.getUniformLocation(shaderProgram, 'uv_texture');
    gl.uniform1i(textureLocation2, 1);


    // camera load
    const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 640, height: 360 } });
    document.getElementById('selfVideo').srcObject = stream;

    const [track] = stream.getVideoTracks();
    const trackProcessor = new MediaStreamTrackProcessor({ track });
    const reader = trackProcessor.readable.getReader();
    let buffer, buffer_y, buffer_uv;
    // debug
    window.catch = false;
    while (true) {
        const readValue = await reader.read();
        if (readValue.done) {
            console.log('DONE')
            break;
        }
        /** @var {VideoFrame} */
        const frame = readValue.value;
        if (frame) {
            const { width, height } = frame.visibleRect;
            const frameSize = width * height;

            if (!buffer || buffer.byteLength !== frame.allocationSize()) {
                console.log(frame.format, frame.codedWidth, frame.codedHeight, frame.allocationSize(), frame.codedRect.toJSON(), frame.visibleRect.toJSON(), frame.colorSpace.toJSON());
                buffer = new Uint8Array(frame.allocationSize());
                buffer_y = new Uint8Array(buffer.buffer, 0, frameSize);
                buffer_uv = new Uint8Array(buffer.buffer, frameSize);

                if (frame.format !== 'NV12') {
                    console.error('Only NV12 frames are supported!');
                    throw new Error('Invalid frame format');
                }
            }

            await frame.copyTo(buffer);

            // Y component
            gl.activeTexture(gl.TEXTURE0);
            gl.texImage2D(gl.TEXTURE_2D, 0, gl.LUMINANCE, width, height, 0 , gl.LUMINANCE, gl.UNSIGNED_BYTE, buffer_y);
            gl.generateMipmap(gl.TEXTURE_2D);

            // UV component
            gl.activeTexture(gl.TEXTURE1);
            gl.texImage2D(gl.TEXTURE_2D, 0, gl.LUMINANCE_ALPHA, width / 2, height / 2, 0 , gl.LUMINANCE_ALPHA, gl.UNSIGNED_BYTE, buffer_uv);
            gl.generateMipmap(gl.TEXTURE_2D);

            gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);

            // debug to extract the real color space transformation matrix
            // ctx.drawImage(frame, 0, 0);
            if (window.catch) {
                window.caught = frame.clone();
                window.catch = false;
            }

            frame.close();
        }
    }
</script>
</body>
</html>
